# GPT From Scratch

**Main Entry Point Website:** [harvestwalukow.github.io/gpt](https://harvestwalukow.github.io/gpt)  
**GitHub Repository:** [github.com/harvestwalukow/gpt](https://github.com/harvestwalukow/gpt)

Hi! I'm currently working on building my own GPT. This is the main entry point website for information and documentation related to this project.

## Current Status

Started with `gpt0.py` as the initial implementation before working through the exercises. The model was trained for 5000 iterations on Google Colab's T4 GPU, taking 1.5 hours.

See the full training log [here](https://colab.research.google.com/drive/1i1ybtH_1UamYDfAfDnQ5YcHOgfmo43br?usp=sharing).

### Training Results

The model achieved:

- Final training loss: 0.8628
- Final validation loss: 1.5670
- Parameters: 10.8M

Sample generated text after training shows Shakespearean-style language generation capabilities, though still in early stages.

### Repository Contents

- gpt0.py - Initial GPT implementation following the code from [Andrej Karpathy](https://github.com/karpathy/ng-video-lecture)
- Python Notebook version of the initial implementation

### Goals

Complete all the EX1, EX2, EX3, and EX4.

---

**Credit:**
[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
